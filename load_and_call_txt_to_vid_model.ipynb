{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install --upgrade transformers accelerate diffusers imageio-ffmpeg"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
    "!pip install git+https://github.com/pytorch/ao.git\n",
    "!pip install optimum-quanto"
   ],
   "id": "d48c17cdc1f329d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# To get started, PytorchAO needs to be installed from the GitHub source and PyTorch Nightly.\n",
    "# Source and nightly installation is only required until next release.\n",
    "\n",
    "import torch\n",
    "from diffusers import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel, CogVideoXPipeline\n",
    "from diffusers.utils import export_to_video\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n",
    "\n",
    "model_id = \"THUDM/CogVideoX-2b\"\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "quantization = int8_weight_only\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "\n",
    "# Load and quantize encoder\n",
    "text_encoder = T5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch_dtype)\n",
    "quantize_(text_encoder, quantization())  # 8-bit quantization\n",
    "\n",
    "# Load and quantize transformer\n",
    "transformer = CogVideoXTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch_dtype)\n",
    "quantize_(transformer, quantization())\n",
    "\n",
    "# Load and quantize VAE\n",
    "vae = AutoencoderKLCogVideoX.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch_dtype)\n",
    "quantize_(vae, quantization())\n",
    "\n",
    "# Create pipeline and run inference\n",
    "pipe = CogVideoXPipeline.from_pretrained(\n",
    "    \"THUDM/CogVideoX-2b\",\n",
    "    text_encoder=text_encoder,\n",
    "    transformer=transformer,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.vae.enable_tiling()\n",
    "\n",
    "\n",
    "prompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\n",
    "output = pipe(\n",
    "    prompt=prompt,\n",
    "    height=256,\n",
    "    width=256,\n",
    "    num_frames=8,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5\n",
    ").frames[0]\n",
    "\n",
    "\n",
    "export_to_video(output, \"output.mp4\", fps=8)"
   ],
   "id": "59704ed3e3321cdf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
